{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch MNIST example dissected\n",
    "\n",
    "In this notebook we'll explore the components of the\n",
    "[PyTorch MNIST example](https://github.com/pytorch/examples/tree/master/mnist)\n",
    "one-by-one.\n",
    "\n",
    "* Part 1: [Loading the data](1_mnist_load.ipynb)\n",
    "* Part 2: [Model components and forward propagation](2_mnist_model.ipynb)\n",
    "* Part 3: [Autodiff and backpropagation](3_mnist_backprop.ipynb) <-- **you are here**\n",
    "* Part 4: [Training the model](4_mnist_train.ipynb)\n",
    "* Part 5: [Visualizing the results](5_mnist_visualize.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Backpropagation\n",
    "\n",
    "Before we start training our model, let's explore the auto-differentiation functionality of PyTorch.\n",
    "\n",
    "In fact, PyTorch has some good [online documentation](https://pytorch.org/docs/stable/notes/autograd.html) on the subject; Below we will focus more on the autograd internals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall in [Part 2](2_mnist_model.ipynb) parameters of the neural net had a flag `require_grad=True`. It indicates that this tensor will participate in backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I * X = tensor([1., 2.], grad_fn=<MvBackward>) requires_grad = True\n"
     ]
    }
   ],
   "source": [
    "i = torch.eye(2, requires_grad=True)\n",
    "x = torch.tensor([1., 2.], requires_grad=False)\n",
    "\n",
    "y = i.matmul(x)\n",
    "\n",
    "print(\"I * X =\", y, \"requires_grad =\", y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If at least one operand has `required_grad` flag set, the result will also have `requires_grad=True`.\n",
    "\n",
    "Note that the result tensor also has the `grad_fn` property - we've seen that in [Part 2](2_mnist_model.ipynb), too. It looks like a function or a callable object. Let's take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on MvBackward object:\n",
      "\n",
      "class MvBackward(object)\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, /, *args, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  register_hook(...)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  metadata\n",
      " |  \n",
      " |  next_functions\n",
      " |  \n",
      " |  requires_grad\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it is a callable indeed. It allso has the `requires_grad` flag, and what looks like a link to the next element (or elements?) in backpropagation chain. Let's feed some data trough the `.__call__()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 6.],\n",
       "         [4., 8.]]), None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.tensor([3., 4.])\n",
    "\n",
    "y.grad_fn(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `y` is the result of the matrix-vector multiplication of matrix `i` and vector `x`, the gradient should propagate to both operands. However, vector `x` has `requires_grad=False`, and therefore we do not propagate data to it. Hence the second element of the result tuple is `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 6.],\n",
       "        [4., 8.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(grad_i, grad_x) = y.grad_fn(z)\n",
    "\n",
    "z.reshape((2,1)).matmul(x.reshape((1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8., grad_fn=<MulBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(2., requires_grad=True)\n",
    "y = torch.tensor(3., requires_grad=True)\n",
    "z = x * 4\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad_fn(torch.tensor(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

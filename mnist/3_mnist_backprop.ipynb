{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch MNIST example dissected\n",
    "\n",
    "In this notebook we'll explore the components of the\n",
    "[PyTorch MNIST example](https://github.com/pytorch/examples/tree/master/mnist)\n",
    "one-by-one.\n",
    "\n",
    "* Part 1: [Loading the data](1_mnist_load.ipynb)\n",
    "* Part 2: [Model components and forward propagation](2_mnist_model.ipynb)\n",
    "* Part 3: [Autodiff and backpropagation](3_mnist_backprop.ipynb) <-- **you are here**\n",
    "* Part 4: [Training the model](4_mnist_train.ipynb)\n",
    "* Part 5: [Visualizing the results](5_mnist_visualize.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Backpropagation\n",
    "\n",
    "Before we start training our model, let's explore the auto-differentiation functionality of PyTorch.\n",
    "\n",
    "In fact, PyTorch has some good [online documentation](https://pytorch.org/docs/stable/notes/autograd.html) on the subject; Below we will focus more on the autograd internals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall in [Part 2](2_mnist_model.ipynb) parameters of the neural net had a flag `require_grad=True`. It indicates that this tensor will participate in backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = tensor(6., grad_fn=<MulBackward>) requires_grad = True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2., requires_grad=True)\n",
    "\n",
    "z = 3 * x\n",
    "\n",
    "print(\"z =\", z, \"requires_grad =\", z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If at least one operand has `required_grad` flag set, the result will also have `requires_grad=True`.\n",
    "\n",
    "Note that the result tensor also has the `grad_fn` property - we've seen that in [Part 2](2_mnist_model.ipynb), too. It looks like a function or a callable object. Let's take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on MulBackward object:\n",
      "\n",
      "class MulBackward(object)\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, /, *args, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  register_hook(...)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  metadata\n",
      " |  \n",
      " |  next_functions\n",
      " |  \n",
      " |  requires_grad\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it is a callable indeed. It allso has the `requires_grad` flag, and what looks like a link to the next element (or elements?) in backpropagation chain. Let's feed some data trough the `.__call__()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad_fn(torch.tensor(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1000), tensor(0.1000))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(2., requires_grad=True)\n",
    "y = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "(x + y).grad_fn(torch.tensor(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3000, grad_fn=<ThMulBackward>),\n",
       " tensor(0.2000, grad_fn=<ThMulBackward>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x * y).grad_fn(torch.tensor(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0333, grad_fn=<DivBackward1>),\n",
       " tensor(-0.0222, grad_fn=<DivBackward1>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x / y).grad_fn(torch.tensor(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

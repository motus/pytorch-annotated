{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch MNIST example dissected\n",
    "\n",
    "In this notebook we'll explore the components of the\n",
    "[PyTorch MNIST example](https://github.com/pytorch/examples/tree/master/mnist)\n",
    "one-by-one.\n",
    "\n",
    "* Part 1: [Loading the data](1_mnist_load.ipynb)\n",
    "* Part 2: [Model components and forward propagation](2_mnist_model.ipynb##2-Building-the-model) <-- **you are here**\n",
    "   * [2.0 Input Layer](#2.0-Input-Layer)\n",
    "   * [2.1 Conv2d](#2.1-Conv2d)\n",
    "      * [2.1.1 Sliding window](#2.1.1-Sliding-window)\n",
    "      * [2.1.2 Aside: Gradient computation](#2.1.2-Aside:-Gradient-computation)\n",
    "   * [2.2 Max pooling](#2.2-Max-pooling)\n",
    "   * [2.3 ReLU](#2.3-ReLU)\n",
    "   * [2.4 Layer 2](#2.4-Layer-2)\n",
    "   * [2.5 2d Dropout](#2.5-2d-Dropout)\n",
    "      * [2.5.1 Aside: Dropout techniques](#2.5.1-Aside:-Dropout-techniques)\n",
    "   * [2.6 Layer 3](#2.6-Layer-3)\n",
    "      * [2.6.1 Linear unit](#2.6.1-Linear-unit)\n",
    "* Part 3: [Autodiff and backpropagation](3_mnist_backprop.ipynb)\n",
    "* Part 4: [Training the model](4_mnist_train.ipynb)\n",
    "* Part 5: [Visualizing the results](5_mnist_visualize.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Building the model\n",
    "\n",
    "In this notebook we'll explore the components of `torch.nn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with copying the model from the original [PyTorch MNIST example](https://github.com/pytorch/examples/tree/master/mnist) verbatim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, any class derived from `nn.Module` should implement the `.forward()` method; everything else is taken care of automagically by the auto-differentiation framework. Pretty neat.\n",
    "\n",
    "The original `x` argument of the `.forward()` method is the **batch** of features generated by the `DataLoader` (see [Part 1](1_mnist_load.ipynb#1.4-Dataset-and-DataLoader) for details). That is, for the MNIST dataset and batch size of 64, the dimensions of the input `x` are\n",
    "\n",
    "    torch.Size([64, 1, 28, 28])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Input Layer\n",
    "\n",
    "First layer of our neural net is defined as\n",
    "\n",
    "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "\n",
    "The combination of 2D convolution, max pooling, and ReLU activation function if very typical for learning image models. Below we will look at each of these three components in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Conv2d\n",
    "\n",
    "First, we feed the input batch through 2D convolution:\n",
    "\n",
    "    self.conv1(x)\n",
    "\n",
    " this convolution is defined with 5x5 kernel and `10` output layers:\n",
    "\n",
    "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "\n",
    "Recall that our input has 1 layer, because it is a greyscale image; hence `1` in `Conv2d()` constructor parameters.\n",
    "\n",
    "Below we will create an instance of `Conv2d` and see what it actually does to the data. We start with a tiny 3x3 input tensor and 3x3 convolution with 4 output layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 1., 1.],\n",
       "          [1., 0., 1.],\n",
       "          [1., 1., 0.]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = 1 - torch.eye(3, dtype=torch.float32).reshape((1, 1, 3, 3))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that NN layers operate on *batches*, hence input data dimensions of [batch of size 1 x 1 color layer x height 3 x width 3]: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(1, 4, kernel_size=3)\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.2802]],\n",
       "\n",
       "         [[-0.0270]],\n",
       "\n",
       "         [[ 0.2794]],\n",
       "\n",
       "         [[-0.4983]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_result = conv(data)\n",
    "conv_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_result.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our convolution converts a 1x3x3 patch of (greyscale) image into a 4x1x1 column of weird numbers. Where do those numbers come from?\n",
    "\n",
    "Let's take a look at the `Conv2d` internals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(params, bias) = list(conv.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.3215,  0.0547, -0.0736],\n",
       "          [-0.1007, -0.1459, -0.0529],\n",
       "          [ 0.1067,  0.0479, -0.1120]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2073,  0.0388, -0.2968],\n",
       "          [ 0.0361,  0.2942,  0.3329],\n",
       "          [-0.2384,  0.2017, -0.2592]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1062, -0.2165, -0.1538],\n",
       "          [ 0.2595, -0.0364, -0.0085],\n",
       "          [ 0.0320,  0.1645, -0.0020]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0402,  0.0258,  0.1012],\n",
       "          [-0.3169, -0.2258,  0.0269],\n",
       "          [ 0.1091, -0.2625,  0.3272]]]], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.2981, -0.1014,  0.2020, -0.1818], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are randomly initialized parameters of our convolution. The second one is a bias vector. (It is possible to have a convolution *without* a bias - that's `bias=False` parameter in the constructor).\n",
    "\n",
    "So what is exactly the computation that `Conv2d` performs on the data during feed forward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2802, -0.0270,  0.2794, -0.4983], grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(params * data).sum(dim=(1, 2, 3)) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woo hoo, we get the same results!\n",
    "\n",
    "That is, for each of 4 output layers of the convolution there is a 3x3 matrix of parameters that we multiply by a 3x3 patch of the input image, and then sum up all elements of that product.\n",
    "\n",
    "Let's do it again, just for one layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3215,  0.0547, -0.0736],\n",
       "         [-0.1007, -0.1459, -0.0529],\n",
       "         [ 0.1067,  0.0479, -0.1120]]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000,  0.0547, -0.0736],\n",
       "          [-0.1007, -0.0000, -0.0529],\n",
       "          [ 0.1067,  0.0479, -0.0000]]]], grad_fn=<ThMulBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[0] * data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2802, grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(params[0] * data).sum() + bias[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of 4 layers of the convolution parameters can be seen as a 3x3 patch of an image that we learn to recognize in the input. Right now it is random, but as we learn (i.e. backpropagate the gradient through it), it will morph into something like, say, a vertical line, or a diagonal gradient. Having 4 layers means that our convolution learns to recognize 4 different image patterns. Our original MNIST model has 10 such layers in its first convolution.\n",
    "\n",
    "Having the bias parameter for each layer allows us to learn image patterns of different intensity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Sliding window\n",
    "\n",
    "In our toy example above input image is the same size as the convolution (3x3). Let's apply our convolution to a larger image now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 1., 1., 1., 1.],\n",
       "          [1., 0., 1., 1., 1.],\n",
       "          [1., 1., 0., 1., 1.],\n",
       "          [1., 1., 1., 0., 1.],\n",
       "          [1., 1., 1., 1., 0.]]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = 1 - torch.eye(5, dtype=torch.float32).reshape((1, 1, 5, 5))\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.2802,  0.3964,  0.2370],\n",
       "          [ 0.3419,  0.2802,  0.3964],\n",
       "          [ 0.4173,  0.3419,  0.2802]],\n",
       "\n",
       "         [[-0.0270, -0.0226,  0.4537],\n",
       "          [-0.1564, -0.0270, -0.0226],\n",
       "          [ 0.5121, -0.1564, -0.0270]],\n",
       "\n",
       "         [[ 0.2794, -0.0770,  0.3151],\n",
       "          [ 0.5721,  0.2794, -0.0770],\n",
       "          [ 0.5009,  0.5721,  0.2794]],\n",
       "\n",
       "         [[-0.4983,  0.2228, -0.4657],\n",
       "          [-0.4093, -0.4983,  0.2228],\n",
       "          [-0.4579, -0.4093, -0.4983]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_result2 = conv(data2)\n",
    "conv_result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the elements at `[0,0]` in each of 4 layers are exactly the same as we had from our 3x3 image:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2802, -0.0270,  0.2794, -0.4983]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_result2[:,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.2802]],\n",
       "\n",
       "         [[-0.0270]],\n",
       "\n",
       "         [[ 0.2794]],\n",
       "\n",
       "         [[-0.4983]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the 2d convolution applies the 3x3 sliding window with the same parameters to each 3x3 patch of the input. In other words, on the forward pass it converts a 1x3x3 patch of input into 4x1x1 column output, and builds a new 4-layer tensor out of these elements.\n",
    "\n",
    "By default, 2d convolution has a **stride** of `1` and **padding** of `0`. That means, we'll move our sliding window by 1 pixel, starting from the [0,0] element and never going *outside* of the input - i.e. applying it first to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 1., 1.],\n",
       "          [1., 0., 1.],\n",
       "          [1., 1., 0.]]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[:, :, 0:3, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1.],\n",
       "          [0., 1., 1.],\n",
       "          [1., 0., 1.]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[:, :, 0:3, 1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and so on, all the way to `data2[:, :, 2:5, 2:5]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Aside: Gradient computation\n",
    "\n",
    "Note that tensors holding the convolution parameters have property `requires_grad=True`.\n",
    "\n",
    "Likewise, the results of the convolution are tensors with `grad_fn` property set.\n",
    "\n",
    "This is where the auto-differentiation magic of PyTorch happens. Later in this course we will dedicate the whole section to discuss it (see [Backpropagation](3_mnist_backprop.ipynb)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Max pooling\n",
    "\n",
    "Next, we apply max pooling to the convolution results, i.e.\n",
    "\n",
    "    F.max_pool2d(self.conv1(x), 2)\n",
    "\n",
    "Here it runs 2x2 sliding window through the data and picks the largest values from each layer within that window. Note that unlike the convolution, by default max pooling windows do *not* overlap.\n",
    "\n",
    "Let's see how it works on some toy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[11., 12., 13., 14., 15., 16.],\n",
       "         [21., 22., 23., 24., 25., 26.],\n",
       "         [31., 32., 33., 34., 35., 36.],\n",
       "         [41., 42., 43., 44., 45., 46.],\n",
       "         [51., 52., 53., 54., 55., 56.],\n",
       "         [61., 62., 63., 64., 65., 66.]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.linspace(10,60,6).reshape((1,6,1)).repeat(1,1,6) + torch.linspace(1,6,6).reshape((1,1,6))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[22., 24., 26.],\n",
       "         [42., 44., 46.],\n",
       "         [62., 64., 66.]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.max_pool2d(x, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[33., 36.],\n",
       "         [63., 66.]]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.max_pool2d(x, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, max pooling effectively scales down the input proportionally to the kernel size. \n",
    "\n",
    "If input dimension is not a multiple of max pooling kernel size, we may lose some information at the edges, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[55.]]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.max_pool2d(x, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply it to the multilayer tensor that is the result of our 2d convolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.2802,  0.3964,  0.2370],\n",
       "          [ 0.3419,  0.2802,  0.3964],\n",
       "          [ 0.4173,  0.3419,  0.2802]],\n",
       "\n",
       "         [[-0.0270, -0.0226,  0.4537],\n",
       "          [-0.1564, -0.0270, -0.0226],\n",
       "          [ 0.5121, -0.1564, -0.0270]],\n",
       "\n",
       "         [[ 0.2794, -0.0770,  0.3151],\n",
       "          [ 0.5721,  0.2794, -0.0770],\n",
       "          [ 0.5009,  0.5721,  0.2794]],\n",
       "\n",
       "         [[-0.4983,  0.2228, -0.4657],\n",
       "          [-0.4093, -0.4983,  0.2228],\n",
       "          [-0.4579, -0.4093, -0.4983]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.4173]],\n",
       "\n",
       "         [[0.5121]],\n",
       "\n",
       "         [[0.5721]],\n",
       "\n",
       "         [[0.2228]]]], grad_fn=<MaxPool2DWithIndicesBackward>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.max_pool2d(conv_result2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that each of 4 layers correspond to some image pattern represented by the convolution parameters. Max pooling, then, allows us to learn the location of each pattern in the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 ReLU\n",
    "\n",
    "Finally, we apply the Rectified Linear Unit activation function to the output:\n",
    "\n",
    "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "\n",
    "and that concludes the first layer of our neural network.\n",
    "\n",
    "`relu()` function simply trims the negative elements of the tensor to zero, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.relu(torch.tensor([-2, -1, 0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, we will not propagate the gradient back to the layers that do not have positive correlation with certain patches of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Layer 2\n",
    "\n",
    "Next layer of our framework is very similar to the first one:\n",
    "\n",
    "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "   \n",
    "Where `conv2` is defined as\n",
    "\n",
    "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "    \n",
    "So we have the same sequence of `Conv2d`, `max_pool2d`, and `relu`.\n",
    "\n",
    "To match the output of the first layer, `conv2` has `10` input dimensions, and `20` output ones. That means, on this stage out neural net will learn 20 different 5x5 patterns, each being a mosaic of 5x5 image components from the layer below. We'll try to visualize these parameters later, after we train our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 2d Dropout\n",
    "\n",
    "One new component that we encounter in the second layer is **2d dropout**, `conv2_drop`. It is defined as\n",
    "\n",
    "    self.conv2_drop = nn.Dropout2d()\n",
    "\n",
    "Let us see what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dropout2d(p=0.4)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop = nn.Dropout2d(p=0.4)\n",
    "drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter `p` specifies the probability of setting random *layers* of the input tensor to zero. The [Deep Learning book](http://www.deeplearningbook.org) suggests `p=0.8` for input layers and `p=0.5` for hidden layers of the net.\n",
    "\n",
    "For each new *batch*, `Dropout2d` will pick different layers at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.2802,  0.3964,  0.2370],\n",
       "          [ 0.3419,  0.2802,  0.3964],\n",
       "          [ 0.4173,  0.3419,  0.2802]],\n",
       "\n",
       "         [[-0.0270, -0.0226,  0.4537],\n",
       "          [-0.1564, -0.0270, -0.0226],\n",
       "          [ 0.5121, -0.1564, -0.0270]],\n",
       "\n",
       "         [[ 0.2794, -0.0770,  0.3151],\n",
       "          [ 0.5721,  0.2794, -0.0770],\n",
       "          [ 0.5009,  0.5721,  0.2794]],\n",
       "\n",
       "         [[-0.4983,  0.2228, -0.4657],\n",
       "          [-0.4093, -0.4983,  0.2228],\n",
       "          [-0.4579, -0.4093, -0.4983]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0450, -0.0376,  0.7561],\n",
      "          [-0.2607, -0.0450, -0.0376],\n",
      "          [ 0.8534, -0.2607, -0.0450]],\n",
      "\n",
      "         [[ 0.4656, -0.1283,  0.5251],\n",
      "          [ 0.9535,  0.4656, -0.1283],\n",
      "          [ 0.8348,  0.9535,  0.4656]],\n",
      "\n",
      "         [[-0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]]]], grad_fn=<FeatureDropoutBackward>)\n"
     ]
    }
   ],
   "source": [
    "drop_result = drop(conv_result2)\n",
    "\n",
    "print(drop_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `Dropout2d` not only zeroes some (random) layers, but also rescales the remaining non-zero values by `1/(1-p)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[1.6667, 1.6667, 1.6667],\n",
       "          [1.6667, 1.6667, 1.6667],\n",
       "          [1.6667, 1.6667, 1.6667]],\n",
       "\n",
       "         [[1.6667, 1.6667, 1.6667],\n",
       "          [1.6667, 1.6667, 1.6667],\n",
       "          [1.6667, 1.6667, 1.6667]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]]]], grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_result / conv_result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1 Aside: Dropout techniques\n",
    "\n",
    "Dropout is a popular regularization technique that promotes independence between elements of the neural net. Since the dropout mask changes from one batch to the next, we effectively create an *ensemble* of models that share some of their parameters.\n",
    "\n",
    "Unlike `Dropout2d`, regular `Dropout` class zeroes out neurons at random, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000,  0.7929,  0.4740],\n",
       "          [ 0.6839,  0.5603,  0.0000],\n",
       "          [ 0.8345,  0.6839,  0.5603]],\n",
       "\n",
       "         [[-0.0000, -0.0452,  0.9073],\n",
       "          [-0.3129, -0.0000, -0.0452],\n",
       "          [ 1.0241, -0.3129, -0.0000]],\n",
       "\n",
       "         [[ 0.5587, -0.1540,  0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000,  1.1442,  0.0000]],\n",
       "\n",
       "         [[-0.9965,  0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000],\n",
       "          [-0.9158, -0.8186, -0.9965]]]], grad_fn=<DropoutBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Dropout(p=0.5)(conv_result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not work well for convolutional nets, where each pixel of the input is strongly correlated with its immediate neigbors. For images, zeroing out random neurons will just slow down the learning process. Layer-wise dropout implemented in `Dropout2d` is much more efficient in such case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Layer 3\n",
    "\n",
    "The layer that follows two convolutional layers is defined as:\n",
    "\n",
    "    self.fc1 = nn.Linear(320, 50)\n",
    "    # ...\n",
    "    x = x.view(-1, 320)\n",
    "    x = F.relu(self.fc1(x))\n",
    "\n",
    "It is a simple linear layer from 320 to 50 elements followed by a familiar ReLU activation function.\n",
    "\n",
    "#### 2.6.1 Aside: Dimensions\n",
    "\n",
    "Where's that dimension of `320` come from? Let's trace how the shape of the data changes as \n",
    "\n",
    "Recall that the input data has dimensions of `torch.Size([64, 1, 28, 28])`. That is, a batch of 64 images with one (greyscale) layer 28x28 pixels each.\n",
    "\n",
    "After the first convolution `Conv2d(1, 10, kernel_size=5)` out comes the tensor of `torch.Size([64, 10, 24, 24])`: stride of 1, padding of 0, and 5x5 kernel produce the result of size `28 - 5 + 1 = 24` in height and width.\n",
    "\n",
    "2d max pooling with 2x2 kernel shrinks the data to half its size in both width and height to `torch.Size([64, 10, 12, 12])`, as `int(dim / kernel_size) = 24/2 = 12`.\n",
    "\n",
    "At the next layer, we apply 5x5 convolution with 20 output layers, `Conv2d(10, 20, kernel_size=5)`. It yields `torch.Size([64, 20, 8, 8])`, sinc `12 - 5 + 1 = 8`.\n",
    "\n",
    "Finally, 2d max pooling with 2x2 kernel halves the the height and width of the input tensor again, making it `torch.Size([64, 20, 4, 4])`. That's a batch of 64 tensors, each having size of 4x4 and with 20 layers, or 320 elements in total (`4 * 4 * 20 = 320`).\n",
    "\n",
    "Now `x.view(-1, 320)` just flattens this tensor into a vector, preserving the first dimension (i.e. the batch size), and we end up with `torch.Size([64, 320])` that can be fed into our linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.2 Linear unit\n",
    "\n",
    "`Linear` is the most fundamental building block of every neural net. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

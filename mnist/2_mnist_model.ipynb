{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch MNIST example dissected\n",
    "\n",
    "In this notebook we'll explore the components of the\n",
    "[PyTorch MNIST example](https://github.com/pytorch/examples/tree/master/mnist)\n",
    "one-by-one.\n",
    "\n",
    "**This is notebook 2 of the multi-part example.** Please see [Part 1 here](1_mnist_load.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the model\n",
    "\n",
    "In this notebook we'll explore the components of `torch.nn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with copying the model from the original [PyTorch MNIST example](https://github.com/pytorch/examples/tree/master/mnist) verbatim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, any class derived from `nn.Module` should implement the `.forward()` method; everything else is taken care of automagically by the auto-differentiation framework. Pretty neat.\n",
    "\n",
    "The original `x` argument of the `.forward()` method is the **batch** of features generated by the `DataLoader` (see [Part 1](1_mnist_load.ipynb#1.4-Dataset-and-DataLoader) for details). That is, for the MNIST dataset and batch size of 64, the dimensions of the input `x` are\n",
    "\n",
    "    torch.Size([64, 1, 28, 28])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Conv2d\n",
    "\n",
    "First, we feed the input batch through 2D convolution:\n",
    "\n",
    "    self.conv1(x)\n",
    "\n",
    " this convolution is defined with 5x5 kernel and `10` output layers:\n",
    "\n",
    "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "\n",
    "Recall that our input has 1 layer, because it is a greyscale image; hence `1` in `Conv2d()` constructor parameters.\n",
    "\n",
    "Below we will create an instance of `Conv2d` and see what it actually does to the data. We start with a tiny 3x3 input tensor and 3x3 convolution with 4 output layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 1., 1.],\n",
       "          [1., 0., 1.],\n",
       "          [1., 1., 0.]]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = 1 - torch.eye(3, dtype=torch.float32).reshape((1, 1, 3, 3))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that NN layers operate on *batches*, hence input data dimensions of [batch of size 1 x 1 color layer x height 3 x width 3]: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(1, 4, kernel_size=3)\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.3212]],\n",
       "\n",
       "         [[-0.3721]],\n",
       "\n",
       "         [[ 0.3503]],\n",
       "\n",
       "         [[-0.1429]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_result = conv(data)\n",
    "conv_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_result.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our convolution converts a 1x3x3 patch of (greyscale) image into a 4x1x1 column of weird numbers. Where do those numbers come from?\n",
    "\n",
    "Let's take a look at the `Conv2d` internals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(params, bias) = list(conv.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.1652,  0.0623,  0.2246],\n",
       "          [ 0.1479, -0.2734, -0.2719],\n",
       "          [-0.3310,  0.1613, -0.0247]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0025, -0.1486, -0.2781],\n",
       "          [ 0.0184, -0.0850, -0.1435],\n",
       "          [ 0.1833,  0.0569, -0.2380]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0323,  0.1469,  0.1247],\n",
       "          [-0.1735, -0.2636,  0.0736],\n",
       "          [-0.1304,  0.2736,  0.2486]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0915, -0.1726,  0.1300],\n",
       "          [-0.0631, -0.0104,  0.2462],\n",
       "          [-0.0241, -0.2866,  0.0535]]]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.3281, -0.0605,  0.0354,  0.0273], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are randomly initialized parameters of our convolution. The second one is a bias vector. (It is possible to have a convolution *without* a bias - that's `bias=False` parameter in the constructor).\n",
    "\n",
    "So what is exactly the computation that `Conv2d` performs on the data during feed forward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3212, -0.3721,  0.3503, -0.1429], grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(params * data).sum(dim=(1, 2, 3)) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woo hoo, we get the same results!\n",
    "\n",
    "That is, for each of 4 output layers of the convolution there is a 3x3 matrix of parameters that we multiply by a 3x3 patch of the input image, and then we just sum up all elements of that product.\n",
    "\n",
    "Let's do it again, just for one layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1652,  0.0623,  0.2246],\n",
       "         [ 0.1479, -0.2734, -0.2719],\n",
       "         [-0.3310,  0.1613, -0.0247]]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000,  0.0623,  0.2246],\n",
       "          [ 0.1479, -0.0000, -0.2719],\n",
       "          [-0.3310,  0.1613, -0.0000]]]], grad_fn=<ThMulBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[0] * data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3212, grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(params[0] * data).sum() + bias[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each of 4 layers of the convolution parameters can be seen as a 3x3 patch of an image that we learn to recognize in the input. Right now it is random, but as we learn (i.e. backpropagate the gradient through it), it will morph into something like, say, a vertical line, or a diagonal gradient. Having 4 layers means that our convolution learns to recognize 4 different image patterns. Our original MNIST model has 10 such layers in its first convolution.\n",
    "\n",
    "Having the bias parameter for each layer allows us to learn image patterns of different intensity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Sliding window\n",
    "\n",
    "In our toy example above input image is the same size as the convolution (3x3). Let's apply our convolution to a larger image now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 1., 1., 1., 1.],\n",
       "          [1., 0., 1., 1., 1.],\n",
       "          [1., 1., 0., 1., 1.],\n",
       "          [1., 1., 1., 0., 1.],\n",
       "          [1., 1., 1., 1., 0.]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = 1 - torch.eye(5, dtype=torch.float32).reshape((1, 1, 5, 5))\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.3212, -0.1208,  0.5194],\n",
       "          [ 0.3980,  0.3212, -0.1208],\n",
       "          [-0.0362,  0.3980,  0.3212]],\n",
       "\n",
       "         [[-0.3721, -0.7678, -0.8759],\n",
       "          [-0.4005, -0.3721, -0.7678],\n",
       "          [-0.4144, -0.4005, -0.3721]],\n",
       "\n",
       "         [[ 0.3503,  0.2675,  0.4980],\n",
       "          [ 0.1472,  0.3503,  0.2675],\n",
       "          [ 0.2429,  0.1472,  0.3503]],\n",
       "\n",
       "         [[-0.1429,  0.3413,  0.0158],\n",
       "          [-0.0819, -0.1429,  0.3413],\n",
       "          [-0.1383, -0.0819, -0.1429]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_result2 = conv(data2)\n",
    "conv_result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the elements at `[0,0]` in each of 4 layers are exactly the same as we had from our 3x3 image:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3212, -0.3721,  0.3503, -0.1429]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_result2[:,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.3212]],\n",
       "\n",
       "         [[-0.3721]],\n",
       "\n",
       "         [[ 0.3503]],\n",
       "\n",
       "         [[-0.1429]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the 2d convolution applies the 3x3 sliding window with the same parameters to each 3x3 patch of the input. In other words, on the forward pass it converts a 1x3x3 patch of input into 4x1x1 column output, and builds a new 4-layer tensor out of these elements.\n",
    "\n",
    "By default, 2d convolution has a **stride** of `1` and **padding** of `0`. That means, we'll move our sliding window by 1 pixel, starting from the [0,0] element and never going *outside* of the input - i.e. applying it first to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 1., 1.],\n",
       "          [1., 0., 1.],\n",
       "          [1., 1., 0.]]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[:, :, 0:3, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1.],\n",
       "          [0., 1., 1.],\n",
       "          [1., 0., 1.]]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[:, :, 0:3, 1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and so on, all the way to `data2[:, :, 2:5, 2:5]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Aside: Gradient computation\n",
    "\n",
    "Note that tensors holding the convolution parameters have property `requires_grad=True`.\n",
    "\n",
    "Likewise, the results of the convolution are tensors with `grad_fn` property set.\n",
    "\n",
    "This is where the auto-differentiation magic of PyTorch happens. Later in this course we will dedicate the whole section to discuss it (see [Backpropagation](3_mnist_backprop.ipynb)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Max pooling\n",
    "\n",
    "Next, we apply max pooling to the convolution results, i.e.\n",
    "\n",
    "    F.max_pool2d(self.conv1(x), 2)\n",
    "\n",
    "This function picks the largest values from each layer.\n",
    "\n",
    "Let's see how it works on our toy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.3212, -0.1208,  0.5194],\n",
       "          [ 0.3980,  0.3212, -0.1208],\n",
       "          [-0.0362,  0.3980,  0.3212]],\n",
       "\n",
       "         [[-0.3721, -0.7678, -0.8759],\n",
       "          [-0.4005, -0.3721, -0.7678],\n",
       "          [-0.4144, -0.4005, -0.3721]],\n",
       "\n",
       "         [[ 0.3503,  0.2675,  0.4980],\n",
       "          [ 0.1472,  0.3503,  0.2675],\n",
       "          [ 0.2429,  0.1472,  0.3503]],\n",
       "\n",
       "         [[-0.1429,  0.3413,  0.0158],\n",
       "          [-0.0819, -0.1429,  0.3413],\n",
       "          [-0.1383, -0.0819, -0.1429]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.3980]],\n",
       "\n",
       "         [[-0.3721]],\n",
       "\n",
       "         [[ 0.3503]],\n",
       "\n",
       "         [[ 0.3413]]]], grad_fn=<MaxPool2DWithIndicesBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.max_pool2d(conv_result2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that each of 4 layers correspond to some image pattern represented by the convolution parameters. Max pooling, then, allows us to learn the location of each pattern in the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 ReLU\n",
    "\n",
    "Finally, we apply the Rectified Linear Unit activation function to the output:\n",
    "\n",
    "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "\n",
    "and that concludes the first layer of our neural network.\n",
    "\n",
    "`relu()` function simply trims the negative elements of the tensor to zero, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.relu(torch.tensor([-2, -1, 0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, we will not propagate the gradient back to the layers that do not have positive correlation with certain patches of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Next layer\n",
    "\n",
    "Next layer of our framework is very similar to the first one:\n",
    "\n",
    "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "   \n",
    "Where `conv2` is defined as\n",
    "\n",
    "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "    \n",
    "So we have the same sequence of `Conv2d`, `max_pool2d`, and `relu`.\n",
    "\n",
    "To match the output of the first layer, `conv2` has `10` input dimensions, and `20` output ones. That means, on this stage out neural net will learn 20 different 5x5 patterns, each being a mosaic of 5x5 image components from the layer below. We'll try to visualize these parameters later, after we train our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Dropout\n",
    "\n",
    "One new component that we encounter in the second layer is dropout, `conv2_drop`. It is defined as\n",
    "\n",
    "    self.conv2_drop = nn.Dropout2d()\n",
    "    \n",
    "Let us see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

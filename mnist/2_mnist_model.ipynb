{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch MNIST example dissected\n",
    "\n",
    "In this notebook we'll explore the components of the\n",
    "[PyTorch MNIST example](https://github.com/pytorch/examples/tree/master/mnist)\n",
    "one-by-one.\n",
    "\n",
    "* Part 1: [Loading the data](1_mnist_load.ipynb)\n",
    "* Part 2: [Model components and forward propagation](2_mnist_model.ipynb##2-Building-the-model) <-- **you are here**\n",
    "   * [2.0 Input Layer](#2.0-Input-Layer)\n",
    "   * [2.1 Conv2d](#2.1-Conv2d)\n",
    "      * [2.1.1 Sliding window](#2.1.1-Sliding-window)\n",
    "      * [2.1.2 Aside: Gradient computation](#2.1.2-Aside:-Gradient-computation)\n",
    "   * [2.2 Max pooling](#2.2-Max-pooling)\n",
    "   * [2.3 ReLU](#2.3-ReLU)\n",
    "   * [2.4 Layer 2](#2.4-Layer-2)\n",
    "   * [2.5 2d Dropout](#2.5-2d-Dropout)\n",
    "      * [2.5.1 Aside: Dropout techniques](#2.5.1-Aside:-Dropout-techniques)\n",
    "   * [2.6 Layer 3](#2.6-Layer-3)\n",
    "      * [2.6.1 Linear unit](#2.6.1-Linear-unit)\n",
    "* Part 3: [Autodiff and backpropagation](3_mnist_backprop.ipynb)\n",
    "* Part 4: [Training the model](4_mnist_train.ipynb)\n",
    "* Part 5: [Visualizing the results](5_mnist_visualize.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Building the model\n",
    "\n",
    "In this notebook we'll explore the components of `torch.nn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with copying the model from the original [PyTorch MNIST example](https://github.com/pytorch/examples/tree/master/mnist) verbatim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, any class derived from `nn.Module` should implement the `.forward()` method; everything else is taken care of automagically by the auto-differentiation framework. Pretty neat.\n",
    "\n",
    "The original `x` argument of the `.forward()` method is the **batch** of features generated by the `DataLoader` (see [Part 1](1_mnist_load.ipynb#1.4-Dataset-and-DataLoader) for details). That is, for the MNIST dataset and batch size of 64, the dimensions of the input `x` are\n",
    "\n",
    "    torch.Size([64, 1, 28, 28])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Input Layer\n",
    "\n",
    "First layer of our neural net is defined as\n",
    "\n",
    "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "\n",
    "The combination of 2D convolution, max pooling, and ReLU activation function if very typical for learning image models. Below we will look at each of these three components in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Conv2d\n",
    "\n",
    "First, we feed the input batch through 2D convolution:\n",
    "\n",
    "    self.conv1(x)\n",
    "\n",
    " this convolution is defined with 5x5 kernel and `10` output layers:\n",
    "\n",
    "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "\n",
    "Recall that our input has 1 layer, because it is a greyscale image; hence `1` in `Conv2d()` constructor parameters.\n",
    "\n",
    "Below we will create an instance of `Conv2d` and see what it actually does to the data. We start with a tiny 3x3 input tensor and 3x3 convolution with 4 output layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 1., 1.],\n",
       "          [1., 0., 1.],\n",
       "          [1., 1., 0.]]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = 1 - torch.eye(3, dtype=torch.float32).reshape((1, 1, 3, 3))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that NN layers operate on *batches*, hence input data dimensions of [batch of size 1 x 1 color layer x height 3 x width 3]: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(1, 4, kernel_size=3)\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.3087]],\n",
       "\n",
       "         [[-0.0022]],\n",
       "\n",
       "         [[-0.6727]],\n",
       "\n",
       "         [[ 1.1125]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_result = conv(data)\n",
    "conv_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_result.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our convolution converts a 1x3x3 patch of (greyscale) image into a 4x1x1 column of weird numbers. Where do those numbers come from?\n",
    "\n",
    "Let's take a look at the `Conv2d` internals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(params, bias) = list(conv.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.0211, -0.1009,  0.1576],\n",
       "          [-0.1557, -0.0796,  0.2922],\n",
       "          [-0.2918,  0.0596, -0.0743]]],\n",
       "\n",
       "\n",
       "        [[[-0.2322,  0.2755,  0.1872],\n",
       "          [ 0.0913,  0.0020, -0.2581],\n",
       "          [-0.0122, -0.0649, -0.0467]]],\n",
       "\n",
       "\n",
       "        [[[-0.3075, -0.3062, -0.3124],\n",
       "          [ 0.0257,  0.1472, -0.1514],\n",
       "          [ 0.1219, -0.0824,  0.0054]]],\n",
       "\n",
       "\n",
       "        [[[-0.2001,  0.1595,  0.1166],\n",
       "          [ 0.0481, -0.2388,  0.0536],\n",
       "          [ 0.2706,  0.1593,  0.0293]]]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.2697, -0.2209,  0.0321,  0.3047], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are randomly initialized parameters of our convolution. The second one is a bias vector. (It is possible to have a convolution *without* a bias - that's `bias=False` parameter in the constructor).\n",
    "\n",
    "So what is exactly the computation that `Conv2d` performs on the data during feed forward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3087, -0.0022, -0.6727,  1.1125], grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(params * data).sum(dim=(1, 2, 3)) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woo hoo, we get the same results!\n",
    "\n",
    "That is, for each of 4 output layers of the convolution there is a 3x3 matrix of parameters that we multiply by a 3x3 patch of the input image, and then sum up all elements of that product.\n",
    "\n",
    "Let's do it again, just for one layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0211, -0.1009,  0.1576],\n",
       "         [-0.1557, -0.0796,  0.2922],\n",
       "         [-0.2918,  0.0596, -0.0743]]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0000, -0.1009,  0.1576],\n",
       "          [-0.1557, -0.0000,  0.2922],\n",
       "          [-0.2918,  0.0596, -0.0000]]]], grad_fn=<ThMulBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[0] * data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3087, grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(params[0] * data).sum() + bias[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of 4 layers of the convolution parameters can be seen as a 3x3 patch of an image that we learn to recognize in the input. Right now it is random, but as we learn (i.e. backpropagate the gradient through it), it will morph into something like, say, a vertical line, or a diagonal gradient. Having 4 layers means that our convolution learns to recognize 4 different image patterns. Our original MNIST model has 10 such layers in its first convolution.\n",
    "\n",
    "Having the bias parameter for each layer allows us to learn image patterns of different intensity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Sliding window\n",
    "\n",
    "In our toy example above input image is the same size as the convolution (3x3). Let's apply our convolution to a larger image now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 1., 1., 1., 1.],\n",
       "          [1., 0., 1., 1., 1.],\n",
       "          [1., 1., 0., 1., 1.],\n",
       "          [1., 1., 1., 0., 1.],\n",
       "          [1., 1., 1., 1., 0.]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = 1 - torch.eye(5, dtype=torch.float32).reshape((1, 1, 5, 5))\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.3087, -0.3876, -0.1919],\n",
       "          [-0.6750, -0.3087, -0.3876],\n",
       "          [-0.6412, -0.6750, -0.3087]],\n",
       "\n",
       "         [[-0.0022, -0.3054, -0.2668],\n",
       "          [-0.2964, -0.0022, -0.3054],\n",
       "          [-0.4662, -0.2964, -0.0022]],\n",
       "\n",
       "         [[-0.6727, -0.7710, -0.9495],\n",
       "          [-0.3701, -0.6727, -0.7710],\n",
       "          [-0.5153, -0.3701, -0.6727]],\n",
       "\n",
       "         [[ 1.1125,  0.4955,  0.4323],\n",
       "          [ 0.4898,  1.1125,  0.4955],\n",
       "          [ 0.5863,  0.4898,  1.1125]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_result2 = conv(data2)\n",
    "conv_result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the elements at `[0,0]` in each of 4 layers are exactly the same as we had from our 3x3 image:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3087, -0.0022, -0.6727,  1.1125]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_result2[:,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.3087]],\n",
       "\n",
       "         [[-0.0022]],\n",
       "\n",
       "         [[-0.6727]],\n",
       "\n",
       "         [[ 1.1125]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the 2d convolution applies the 3x3 sliding window with the same parameters to each 3x3 patch of the input. In other words, on the forward pass it converts a 1x3x3 patch of input into 4x1x1 column output, and builds a new 4-layer tensor out of these elements.\n",
    "\n",
    "By default, 2d convolution has a **stride** of `1` and **padding** of `0`. That means, we'll move our sliding window by 1 pixel, starting from the [0,0] element and never going *outside* of the input - i.e. applying it first to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 1., 1.],\n",
       "          [1., 0., 1.],\n",
       "          [1., 1., 0.]]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[:, :, 0:3, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1.],\n",
       "          [0., 1., 1.],\n",
       "          [1., 0., 1.]]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[:, :, 0:3, 1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and so on, all the way to `data2[:, :, 2:5, 2:5]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Aside: Gradient computation\n",
    "\n",
    "Note that tensors holding the convolution parameters have property `requires_grad=True`.\n",
    "\n",
    "Likewise, the results of the convolution are tensors with `grad_fn` property set.\n",
    "\n",
    "This is where the auto-differentiation magic of PyTorch happens. Later in this course we will dedicate the whole section to discuss it (see [Backpropagation](3_mnist_backprop.ipynb)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Max pooling\n",
    "\n",
    "Next, we apply max pooling to the convolution results, i.e.\n",
    "\n",
    "    F.max_pool2d(self.conv1(x), 2)\n",
    "\n",
    "This function picks the largest values from each layer.\n",
    "\n",
    "Let's see how it works on our toy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.3087, -0.3876, -0.1919],\n",
       "          [-0.6750, -0.3087, -0.3876],\n",
       "          [-0.6412, -0.6750, -0.3087]],\n",
       "\n",
       "         [[-0.0022, -0.3054, -0.2668],\n",
       "          [-0.2964, -0.0022, -0.3054],\n",
       "          [-0.4662, -0.2964, -0.0022]],\n",
       "\n",
       "         [[-0.6727, -0.7710, -0.9495],\n",
       "          [-0.3701, -0.6727, -0.7710],\n",
       "          [-0.5153, -0.3701, -0.6727]],\n",
       "\n",
       "         [[ 1.1125,  0.4955,  0.4323],\n",
       "          [ 0.4898,  1.1125,  0.4955],\n",
       "          [ 0.5863,  0.4898,  1.1125]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.3087]],\n",
       "\n",
       "         [[-0.0022]],\n",
       "\n",
       "         [[-0.3701]],\n",
       "\n",
       "         [[ 1.1125]]]], grad_fn=<MaxPool2DWithIndicesBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.max_pool2d(conv_result2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that each of 4 layers correspond to some image pattern represented by the convolution parameters. Max pooling, then, allows us to learn the location of each pattern in the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 ReLU\n",
    "\n",
    "Finally, we apply the Rectified Linear Unit activation function to the output:\n",
    "\n",
    "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "\n",
    "and that concludes the first layer of our neural network.\n",
    "\n",
    "`relu()` function simply trims the negative elements of the tensor to zero, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.relu(torch.tensor([-2, -1, 0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, we will not propagate the gradient back to the layers that do not have positive correlation with certain patches of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Layer 2\n",
    "\n",
    "Next layer of our framework is very similar to the first one:\n",
    "\n",
    "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "   \n",
    "Where `conv2` is defined as\n",
    "\n",
    "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "    \n",
    "So we have the same sequence of `Conv2d`, `max_pool2d`, and `relu`.\n",
    "\n",
    "To match the output of the first layer, `conv2` has `10` input dimensions, and `20` output ones. That means, on this stage out neural net will learn 20 different 5x5 patterns, each being a mosaic of 5x5 image components from the layer below. We'll try to visualize these parameters later, after we train our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 2d Dropout\n",
    "\n",
    "One new component that we encounter in the second layer is **2d dropout**, `conv2_drop`. It is defined as\n",
    "\n",
    "    self.conv2_drop = nn.Dropout2d()\n",
    "\n",
    "Let us see what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dropout2d(p=0.4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop = nn.Dropout2d(p=0.4)\n",
    "drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter `p` specifies the probability of setting random *layers* of the input tensor to zero. The [Deep Learning book](http://www.deeplearningbook.org) suggests `p=0.8` for input layers and `p=0.5` for hidden layers of the net.\n",
    "\n",
    "For each new *batch*, `Dropout2d` will pick different layers at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.3087, -0.3876, -0.1919],\n",
       "          [-0.6750, -0.3087, -0.3876],\n",
       "          [-0.6412, -0.6750, -0.3087]],\n",
       "\n",
       "         [[-0.0022, -0.3054, -0.2668],\n",
       "          [-0.2964, -0.0022, -0.3054],\n",
       "          [-0.4662, -0.2964, -0.0022]],\n",
       "\n",
       "         [[-0.6727, -0.7710, -0.9495],\n",
       "          [-0.3701, -0.6727, -0.7710],\n",
       "          [-0.5153, -0.3701, -0.6727]],\n",
       "\n",
       "         [[ 1.1125,  0.4955,  0.4323],\n",
       "          [ 0.4898,  1.1125,  0.4955],\n",
       "          [ 0.5863,  0.4898,  1.1125]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.5145, -0.6459, -0.3198],\n",
      "          [-1.1250, -0.5145, -0.6459],\n",
      "          [-1.0687, -1.1250, -0.5145]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[ 1.8542,  0.8258,  0.7206],\n",
      "          [ 0.8163,  1.8542,  0.8258],\n",
      "          [ 0.9772,  0.8163,  1.8542]]]], grad_fn=<FeatureDropoutBackward>)\n"
     ]
    }
   ],
   "source": [
    "drop_result = drop(conv_result2)\n",
    "\n",
    "print(drop_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `Dropout2d` not only zeroes some (random) layers, but also rescales the remaining non-zero values by `1/(1-p)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.6667, 1.6667, 1.6667],\n",
       "          [1.6667, 1.6667, 1.6667],\n",
       "          [1.6667, 1.6667, 1.6667]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[1.6667, 1.6667, 1.6667],\n",
       "          [1.6667, 1.6667, 1.6667],\n",
       "          [1.6667, 1.6667, 1.6667]]]], grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_result / conv_result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1 Aside: Dropout techniques\n",
    "\n",
    "Dropout is a popular regularization technique that promotes independence between elements of the neural net. Since the dropout mask changes from one batch to the next, we effectively create an *ensemble* of models that share some of their parameters.\n",
    "\n",
    "Unlike `Dropout2d`, regular `Dropout` class zeroes out neurons at random, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0000, -0.0000, -0.0000],\n",
       "          [-1.3499, -0.0000, -0.7751],\n",
       "          [-0.0000, -1.3499, -0.0000]],\n",
       "\n",
       "         [[-0.0000, -0.6109, -0.5336],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0044]],\n",
       "\n",
       "         [[-0.0000, -1.5420, -0.0000],\n",
       "          [-0.7402, -1.3454, -1.5420],\n",
       "          [-1.0306, -0.7402, -0.0000]],\n",
       "\n",
       "         [[ 2.2251,  0.0000,  0.8647],\n",
       "          [ 0.9795,  2.2251,  0.9910],\n",
       "          [ 0.0000,  0.0000,  2.2251]]]], grad_fn=<DropoutBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Dropout(p=0.5)(conv_result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not work well for convolutional nets, where each pixel of the input is strongly correlated with its immediate neigbors. For images, zeroing out random neurons will just slow down the learning process. Layer-wise dropout implemented in `Dropout2d` is much more efficient in such case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Layer 3\n",
    "\n",
    "The layer that follows two convolutional layers is defined as:\n",
    "\n",
    "    self.fc1 = nn.Linear(320, 50)\n",
    "    # ...\n",
    "    x = x.view(-1, 320)\n",
    "    x = F.relu(self.fc1(x))\n",
    "\n",
    "It is a simple linear layer from 320 to 50 elements followed by familiar ReLU activation function.\n",
    "\n",
    "Where's that dimension of `320` comes from?\n",
    "\n",
    "Recall that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.1 Linear unit\n",
    "\n",
    "`Linear` is the most fundamental building block of every neural net. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
